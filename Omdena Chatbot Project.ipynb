{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78536521-ec92-4128-8f78-f2dec79f4368",
   "metadata": {},
   "source": [
    "##### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af5bcca-8636-43a6-b99b-7b849b6b6120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Keith\n",
      "[nltk_data]     Martins\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Keith\n",
      "[nltk_data]     Martins\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Keith\n",
      "[nltk_data]     Martins\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk #Natural languange Processing tool\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer #model for stemming words\n",
    "stemmer = LancasterStemmer()\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as num\n",
    "from autocorrect import Speller\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab651cb-cf09-4168-86ad-1fa9395f7de4",
   "metadata": {},
   "source": [
    "##### JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f18d8690-fbd5-4eb9-95e4-f2e53e16fb64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "the_data ={\"intents\": [\n",
    "        {\"tag\" : \"greeting\",\n",
    "         \"patterns\": [\"Hi\", \"Hi there\", \"Hey\", \"hey\", \"hi\", \"hi there\", \"how are you\", \"How are you\", \"sasa\", \"SaSa\", \"sema\", \"Sema\",\n",
    "            \"Habari\", \"Habari yako\", \"uko aje\", \"Ukoje\", \"hello\", \"Hello\", \"Good evening\", \"Good morning\", \"Good afternoon\",\n",
    "            \"Morning\", \"evening\",     \"afternoon\", \"hello there\", \"Hello there\", \"mambo\", \"Mambo\"],\n",
    "          \"responses\":[\"Hi stranger\", \"poa\", \"Hi too\", \"hi too\", \"hello\", \"Hola\", \"hello, how can I help you?\", \"Poa\", \"Mzuri\",\"mzuri\"]\n",
    "        },\n",
    "        {\"tag\" : \"goodbye\",\n",
    "         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\", \"Ok bye\", \"Bye Bye\", \"bye\", \"bye bye\", \"okay\", \"Okay\",\"wakati mwema\"],\n",
    "         \"responses\" : [\"See you!\", \"Have a nice day\",\"shinda poa\", \"sure bye\", \"Sure byeeeee\", \"Same to you\"]\n",
    "        },\n",
    "        {\"tag\" : \"thanks\",\n",
    "         \"patterns\" : [\"Thanks\", \"Thank you\", \"That was helpful\", \"Awesome\", \"Thanks for your help\", \"asanti\", \"thank you\", \"thank you so much\",\"Nashukuru\"],\n",
    "        \"responses\" : [\"Happy to help\", \"Any time!\",\"my pleasure\",\"You are welcome\", \"Sure, see you then\", \"My Pleasure\", \"Welcome\"]\n",
    "         },\n",
    "         {\"tag\" : \"noanswer\",\n",
    "          \"patterns\" : [],\n",
    "          \"responses\" : [\"Sorry, I can't understand you\", \"Please give me mmore information\",\n",
    "            \"Kindly elaborate on your issue\", \"Provide more information please\", \"Pardon please\", \"Kindly explain more in new terms\"]\n",
    "         },\n",
    "         {\"tag\":\"name\",\n",
    "          \"patterns\":[\"what is your name?\",\"your name please\",\"name please\",\"Your Name please\",\"Can you tell me your name\"],\n",
    "          \"responses\":[\"am PrinceBot\", \"call me PrinceBot\",\"Okay, am PrinceBot\",\"My name is PrinceBot\", \"PrinceBot and what about your name\"]\n",
    "         },\n",
    "        {\"tag\" : \"need\",\n",
    "         \"patterns\" : [\"I am..., I want your help\", \"My name is...,please help\", \"Am called...help me here please\"],\n",
    "         \"responses\": [\"Good to know your name, how can I help you\", \"Great, how can I be of help to you?\", \"Okay, state the issue you want me to help you, please\"]\n",
    "        },\n",
    "\n",
    "        \n",
    "\n",
    "        {\"tag\" : \"options\", \n",
    "\n",
    "         \"patterns\": [\"Can you give me information about DeKUT?\", \"Can you tell me about Dedan Kimathi University of Technology?\",\n",
    "\n",
    "            \"dekut\", \"Dekut\", \"DEKUT\", \"DeKUT\", \"Dedan Kimathi University of Technology\", \"Tell me about DeKUT\", \"What about DeKUT\",\n",
    "\n",
    "            \"How is DeKUT\", \"how is dekut\", \"HOW IS DEKUT\", \"Explain to me about dekut\", \"Explain to me about DeKUT\", \"I want information about DeKUT\", \n",
    "\n",
    "            \"Tell me about Dedan Kimathi University\", \"general information about Dedan Kimathi University\", \"General information about Dedan Kimathi University\", \"General information about Dedan Kimathi University of Technology\"],\n",
    "\n",
    "         \"responses\": [\"Dedan Kimathi University of Technology(DeKUT) is a public, coeducational technological University in Africa-Kenya. First university to be established under the new universities act of 2012 and 8th public university in the country. It has two compuses: Main campus located in Nyeri along Nyahururu-Mweiga road and Nairobi CBD campus. DeKUT offers certificate, diploma, undergraduate, masters and PHD programmes in various field.\" ]\n",
    "\n",
    "        },\n",
    "\n",
    "                             \n",
    "       {\"tag\":\"club\",\n",
    "        \"patterns\":[\"Clubs at Dedan Kimathi University of Technology\",\"clubs at DeKUT\",\"List of clubs at Dedan Kimathi University\",\"How many clubs are at Dedan Kimathi University of Technology\",\"clubs\",\n",
    "            \"Clubs\",\"Give me clubs at Dedan Kimathi University of Technology\",\"Clubs found at DeKUT\"],\n",
    "        \"responses\":[\"International Association Exchange of Students for Technical Experience(IAESTE, Contact)\",\n",
    "                     \"DeKUT Innovators Club(Contact: )\",\n",
    "                     \"Google Developers Students Club_DeKUT (Contact: )\",\n",
    "                     \"DeKUT Ajira Digital Club (Contact: )\",\n",
    "                     \"Data Science and Artificial Intelligence Club (Contact: )\",\n",
    "                     \"Microsoft Learn Students Ambassandors DeKUT Club (Contact: )\"]\n",
    "        \n",
    "        },\n",
    "       {\"tag\" : \"location\",\n",
    "\n",
    "        \"patterns\": [\"Where is DeKUT located\", \"location\", \"how can I find Dedan Kimathi University of Technology\", \"Location of DeKUT.\", \n",
    "\n",
    "            \"location of Dedan Kimathi Universitiy of Technology\", \"Which city is Dedan Kimathi University of Techology found.\", \"where is Dedan Kimathi University of Technology located.\"],\n",
    "\n",
    "         \"responses\" : [\" DeKUT has two campuses:  Main Campus located in Nyeri county,Kenya along Nyahururu-Mweiga highway.  Nairobi CBD Campus located at Pension Towers in Loita Street, nd, th, th floor.\"]\n",
    "\n",
    "        },\n",
    "\n",
    "        \n",
    "\n",
    "       {\"tag\" : \"programmes\",\n",
    "\n",
    "        \"patterns\" :[\"What are the programmes offerred at Dedan Kimathi University of Technology.\", \"Courses offered at DeKUT.\", \"Courses taught at DeKUT\", \"List of courses at Dedan Kimathi University of Technology.\", \n",
    "\n",
    "            \"Courses offered at Dedan Kimathi University of Technology\", \"Courses offered at Dedan Kimathi University.\"],\n",
    "\n",
    "        \"responses\":[\"Dekut offers Postgraduate Programmes ,Undergraduate Programmes,Diploma Courses,Certificate Courses\"]\n",
    "\n",
    "        },\n",
    "\n",
    "       {\"tag\" : \"postgraduate\",\n",
    "\n",
    "        \"patterns\":[\"What are the postgraduate programmes\", \"postgraduate programmes\", \"Postgraduate Programmes\", \"Postgraduate programmes\",\n",
    "\n",
    "            \"Postgraduate programmes offered in DeKUT\",\"PhD courses\",\"masters courses\",\"Masters programmes\",\"Masters courses\"],\n",
    "\n",
    "        \"responses\" : [\"Post graduate programmes currently offered by DEKUT are PhD in Mechanical Engineering, PhD in Geomatics & Geospatial Information Science(GeGIS), PhD in Food Science & Technology,PhD in Computer Science, PhD in Business Administration,Master of Science in Geothermal Energy Technology,Master of Science in Telecommunication Engineering\"]\n",
    "\n",
    "        },\n",
    "       {\"tag\": \"undergraduate\",\n",
    "\n",
    "        \"patterns\":[\"What are the undergraduate programmes\",\"Degree courses\",\"degree courses\",\"undergraduate programmes\",\"Undergradaute programmes\",\n",
    "\n",
    "            \"Undergraduate Programmes\",\"Undergraduate programmes offered in DeKUT\",\"List to me Undergraduate Programmes\",\"Undergraduate courses\"],\n",
    "\n",
    "        \"responses\" :[\"BSc in Mechatronic Engineering\",\"BSc in Mechanical Engineering\",\"BSc in Computer Science\",\"BSc in Acturial Science\",\"BSc in Civil Engineering\", \n",
    "\n",
    "            \"Bachelor in Commerce\",\"Bachelor of Purchasing and Supplies Management\",\"BSc in Geology\",\"BSc in Industrial Chemistry\",\"BSc in Information Technology\",\"BSc in Nursing (Direct Entry)\"]\n",
    "         },\n",
    "\n",
    "        {\"tag\":\"diploma\",\n",
    "         \"patterns\":[\"Give me diploma programmes at Dedan Kimathi University of Technology\",\"diploma programmes\",\"Diploma programmes\",\"What are the diploma programmes offered at DeKUT?\",\"Diploma courses\"],\n",
    "         \"responses\":[\"Diploma in Leather Technology\",\"Diploma in Information Technology\",\"Diploma in Security Management\",\"Diploma in Coffee Technology & Cupping\",\"Diploma in Building Technology\",\"Diploma in Fashion Design and Interior Decoration\"]\n",
    "         },\n",
    "\n",
    "        {\"tag\":\"certificate\",\n",
    "         \"patterns\":[\"Give me certificate programmes at Dedan Kimathi University of Technology\",\"certificate programmes\",\"Certificate programmes\",\"What are the certificate programmes offered at DeKUT?,Certificate courses\"],\n",
    "         \"responses\":[\"Certificate in Information Technology\",\"Certificate in Building Technology\",\"Certificate in Electrical & Electronics & Engineering(Power Option)\",\"Certificate in Coffee Technology & Quality Management\",\"Certificate in Welding,Metal Works & Design\"]\n",
    "         },\n",
    "       \n",
    "        {\"tag\":\"opportunity\",\n",
    "         \"patterns\":[\"What are the opportunities at Dedan Kimathi University of Technology\",\"Opportunites at Dedan Kimathi University of Technology\",\"Any job opportunites at Dedan Kimathi University of Technology\",\n",
    "            \"Jobs at Dedan Kimathi University of Technology\",\"Any Jobs at DeKUT?\",\"Can I get a job at DeKUT\",\"How can I get a job at DeKUT\",\"How can I get employed at Dedan Kimathi University of Technology\",\n",
    "            \"employment at DeKUT\",\"Any Tender at DeKUT\",\"How can I get a tender at Dedan Kimathi University of Technology\"],\n",
    "         \"responses\":[\"Job opportunities, internships and Tenders are always posted on the University website. Kindly check for any open opportunities and apply for one that interest you.\",\n",
    "            \"For Jobs, Internships and Tender available in the institution Kindly check the University website. If any is open, follow the given procedures to apply\"]\n",
    "        },\n",
    "       \n",
    "       {\"tag\":\"club\",\n",
    "        \"patterns\":[\"Clubs at Dedan Kimathi University of Technology\",\"clubs at DeKUT\",\"List of clubs at Dedan Kimathi University\",\"How many clubs are at Dedan Kimathi University of Technology\",\"clubs\",\n",
    "            \"Clubs\",\"Give me clubs at Dedan Kimathi University of Technology\",\"Clubs found at DeKUT\"],\n",
    "        \"responses\":[\"International Association Exchange of Students for Technical Experience(IAESTE, Contact)\",\n",
    "                     \"DeKUT Innovators Club(Contact: )\",\n",
    "                     \"Google Developers Students Club_DeKUT (Contact: )\",\n",
    "                     \"DeKUT Ajira Digital Club (Contact: )\",\n",
    "                     \"Data Science and Artificial Intelligence Club (Contact: )\",\n",
    "                     \"Microsoft Learn Students Ambassandors DeKUT Club (Contact: )\"]\n",
    "        \n",
    "        }\n",
    "]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c11bb-9661-4b14-8d49-a147e7807a71",
   "metadata": {},
   "source": [
    "##### Word Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31174bad-3939-4e6c-aabf-f453abb7c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in the_data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        characters=sorted(list(set(pattern)))\n",
    "        n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "        char_to_n = {char:n for n, char in enumerate(characters)} \n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        length = len(pattern)\n",
    "        seq_length = 100\n",
    "        \n",
    "        for i in range(0, length-seq_length, 1):\n",
    "            sequence = the_data[i:i+seq_length]\n",
    "            label = the_data[i + seq_length]\n",
    "            X.append([char_to_n[char] for char in sequence])\n",
    "            y.append(char_to_n[label])\n",
    "            \n",
    "            X_mod = np.reshape(X, len(X), seq_length,1)\n",
    "            X_mod = X_mod/float(len(characters))\n",
    "            y_mod = np_utils.to_categorical(y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08308c95-7c2c-456e-b80d-cf0f0c006fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer() \n",
    "ourClasses = []\n",
    "newWords = []\n",
    "documentX = []\n",
    "documentY = []\n",
    "\n",
    "for intent in the_data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        ournewTkns = nltk.word_tokenize(pattern)\n",
    "        newWords.extend(ournewTkns)\n",
    "        documentX.append(pattern)\n",
    "        documentY.append(intent['tag'])\n",
    "        \n",
    "        if intent['tag'] not in ourClasses:\n",
    "            ourClasses.append(intent['tag'])\n",
    "    \n",
    "newWords = [lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation]\n",
    "newWords = sorted(set(newWords))\n",
    "ourClasses = sorted(set(ourClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ed1cf6a-db17-4169-8314-f67672179d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData = [] # training list array\n",
    "outEmpty = [0] * len(ourClasses)\n",
    "\n",
    "# bow model\n",
    "for idx, doc in enumerate(documentX):\n",
    "    bagOfwords = []\n",
    "    text = lm.lemmatize(doc.lower())\n",
    "    \n",
    "    for word in newWords:\n",
    "        bagOfwords.append(1) if word in text else bagOfwords.append(0)\n",
    "    \n",
    "    outputRow = list(outEmpty)\n",
    "    outputRow[ourClasses.index(documentY[idx])] = 1\n",
    "    trainingData.append([bagOfwords, outputRow])\n",
    "\n",
    "random.shuffle(trainingData)\n",
    "trainingData = num.array(trainingData, dtype=object)\n",
    "\n",
    "x = num.array(list(trainingData[:, 0]))\n",
    "y = num.array(list(trainingData[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236dca71-4aac-4c9b-9d75-923c0ae084c0",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2a4429e-857b-407b-984c-d40c1209a0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 128)               13312     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,118\n",
      "Trainable params: 15,118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 7s 36ms/step - loss: 2.5635 - accuracy: 0.2128\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.9891 - accuracy: 0.5035\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5269 - accuracy: 0.6383\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1502 - accuracy: 0.6879\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7996 - accuracy: 0.8156\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6099 - accuracy: 0.8511\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4142 - accuracy: 0.9362\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3168 - accuracy: 0.9574\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.9504\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9716\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.9858\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9858\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.9929\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9858\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9858\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9929\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9929\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.9929\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9858\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.9929\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9929\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9929\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.6576e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.8612e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.2551e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.8376e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.8320e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.7854e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.6443e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.3105e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.8095e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.8767e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.8125e-04 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.7043e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.2627e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.6318e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 3.8059e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.4807e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.1928e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 7.2859e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.8995e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.9290e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.5119e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.7039e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.0667e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 7.6881e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.4639e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 7.9629e-04 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.2273e-04 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 8.4143e-04 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.3409e-04 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.7361e-04 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.9285e-04 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.2448e-04 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.8194e-04 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.7602e-04 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 5.4115e-04 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.1544e-04 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.5691e-04 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 8.0912e-04 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.3539e-04 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 8.0092e-04 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 7.6412e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 5.0057e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 3.5560e-04 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.9240e-04 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 4.0851e-04 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.8814e-04 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 4.7776e-04 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.9283e-04 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.3130e-04 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 7.0236e-04 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 6.7770e-04 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 6.7995e-04 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.0852e-04 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 3.9934e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 5.8061e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.7210e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.9846e-04 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.2407e-04 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 6.7819e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.7821e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.6311e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2822640cac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iShape = (len(x[0]),)\n",
    "oShape = len(y[0])\n",
    "\n",
    "The_Model = Sequential()\n",
    "The_Model.add(Dense(128, input_shape=iShape, activation=\"relu\"))\n",
    "The_Model.add(Dropout(0.5))\n",
    "The_Model.add(Dense(oShape, activation = \"softmax\"))\n",
    "md = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "\n",
    "The_Model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=md,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(The_Model.summary())\n",
    "The_Model.fit(x, y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fbc10-ca75-4e2f-9ed1-14c07acab6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034146b-9ec5-47cf-85be-1d9049690503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 176ms/step\n",
      "Poa\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tell me about dekut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "Dedan Kimathi University of Technology(DeKUT) is a public, coeducational technological University in Africa-Kenya. First university to be established under the new universities act of 2012 and 8th public university in the country. It has two compuses: Main campus located in Nyeri along Nyahururu-Mweiga road and Nairobi CBD campus. DeKUT offers certificate, diploma, undergraduate, masters and PHD programmes in various field.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tell me about degree programs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "Dedan Kimathi University of Technology(DeKUT) is a public, coeducational technological University in Africa-Kenya. First university to be established under the new universities act of 2012 and 8th public university in the country. It has two compuses: Main campus located in Nyeri along Nyahururu-Mweiga road and Nairobi CBD campus. DeKUT offers certificate, diploma, undergraduate, masters and PHD programmes in various field.\n"
     ]
    }
   ],
   "source": [
    "def ourText(text):\n",
    "  spell = Speller()\n",
    "  newtkns = nltk.word_tokenize(spell(text.translate(str.maketrans('', '', string.punctuation))))\n",
    "  newtkns = [lm.lemmatize(word) for word in newtkns]\n",
    "  return newtkns\n",
    "\n",
    "\n",
    "def wordBag(text, vocab):\n",
    "  newtkns = ourText(text)\n",
    "  bagOwords = [0] * len(vocab)\n",
    "  for w in newtkns:\n",
    "    for idx, word in enumerate(vocab):\n",
    "      if word == w:\n",
    "        bagOwords[idx] = 1\n",
    "  return num.array(bagOwords)\n",
    "\n",
    "\n",
    "def Pclass(text, vocab, labels):\n",
    "  bagOwords = wordBag(text, vocab)\n",
    "  ourResult = The_Model.predict(num.array([bagOwords]))[0]\n",
    "  newThresh = 0.2\n",
    "  yp = [[idx, res] for idx, res in enumerate(ourResult) if res > newThresh]\n",
    "\n",
    "  yp.sort(key=lambda x: x[1], reverse=True)\n",
    "  newList = []\n",
    "  for r in yp:\n",
    "    newList.append(labels[r[0]])\n",
    "  return newList\n",
    "\n",
    "def getRes(firstlist, fJson):\n",
    "  tag = firstlist[0]\n",
    "  listOfIntents = fJson[\"intents\"]\n",
    "  for i in listOfIntents:\n",
    "    if i[\"tag\"] == tag:\n",
    "      ourResult = random.choice(i[\"responses\"])\n",
    "      break\n",
    "  return ourResult\n",
    "\n",
    "\n",
    "while True:\n",
    "    newMessage = input(\"\")\n",
    "    intents = Pclass(newMessage, newWords, ourClasses)\n",
    "    ourResult = getRes(intents, the_data)\n",
    "    print(ourResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539bed18-dafb-44e4-9572-7ae97c37dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7dcd8a-6bc5-47bd-8c26-5741ca769328",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/config.json not found in cache or force_download set to True, downloading to C:\\Users\\Keith Martins\\.cache\\huggingface\\transformers\\tmpjmayhncy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4e271df2e646959453ce64f74cb875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/config.json in cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\d6a784b31cbe212ac3dabbc78bc4e454cf9d8a1b11ff2ef1ba1c7497f2bbfb33.7a3e093739f407bcc025e64dfc5244f91bc573ed4285cba53de90f960cbce58e\n",
      "creating metadata file for C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\d6a784b31cbe212ac3dabbc78bc4e454cf9d8a1b11ff2ef1ba1c7497f2bbfb33.7a3e093739f407bcc025e64dfc5244f91bc573ed4285cba53de90f960cbce58e\n",
      "loading configuration file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/config.json from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\d6a784b31cbe212ac3dabbc78bc4e454cf9d8a1b11ff2ef1ba1c7497f2bbfb33.7a3e093739f407bcc025e64dfc5244f91bc573ed4285cba53de90f960cbce58e\n",
      "Model config PegasusConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\Keith Martins\\.cache\\huggingface\\transformers\\tmpa5v9cr05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ace7398a5784ceb9535e287ee915b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/pytorch_model.bin in cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\227d9f57dd37c1f4f5be77ccf5f896e7551ec64db897d21c4613d235eb9cc73a.9e93d391568a0e6e2b8408cd2311ebfa97682e0e0bd8b83a631ab7ad40e93905\n",
      "creating metadata file for C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\227d9f57dd37c1f4f5be77ccf5f896e7551ec64db897d21c4613d235eb9cc73a.9e93d391568a0e6e2b8408cd2311ebfa97682e0e0bd8b83a631ab7ad40e93905\n",
      "loading weights file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/pytorch_model.bin from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\227d9f57dd37c1f4f5be77ccf5f896e7551ec64db897d21c4613d235eb9cc73a.9e93d391568a0e6e2b8408cd2311ebfa97682e0e0bd8b83a631ab7ad40e93905\n",
      "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
      "\n",
      "All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at tuner007/pegasus_paraphrase.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n",
      "https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to C:\\Users\\Keith Martins\\.cache\\huggingface\\transformers\\tmpcwola8od\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ed1d1775ca4b8aae938c21343184c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/86.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/tokenizer_config.json in cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\3f72dece4d3fcab1480f78a2c2a1f6ab591bb9b77ec8b049f8faa418bd50526d.f96255d0af339b1faae306ed2925a98f5266a4ebbf793a8bca5107f6e0f876dd\n",
      "creating metadata file for C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\3f72dece4d3fcab1480f78a2c2a1f6ab591bb9b77ec8b049f8faa418bd50526d.f96255d0af339b1faae306ed2925a98f5266a4ebbf793a8bca5107f6e0f876dd\n",
      "https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/spiece.model not found in cache or force_download set to True, downloading to C:\\Users\\Keith Martins\\.cache\\huggingface\\transformers\\tmpb37y73q1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0db8dff4d894a2492ba69399f9976b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/spiece.model in cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\b3949a7257f9b4eaaf4f7785be079d89e4fec7d1c3763a58e6a2743635877d1f.1acf68c74589da6c7fa3548093824dfc450a54637f4356929bbfea7e294a68f8\n",
      "creating metadata file for C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\b3949a7257f9b4eaaf4f7785be079d89e4fec7d1c3763a58e6a2743635877d1f.1acf68c74589da6c7fa3548093824dfc450a54637f4356929bbfea7e294a68f8\n",
      "https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to C:\\Users\\Keith Martins\\.cache\\huggingface\\transformers\\tmp27gebhfj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248075638dff4c49ac41c0063d34ceeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/special_tokens_map.json in cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\1ff5597b098cfe2ee6d3b0c4b3e94e549fcb86f2e033146119d1591f6e4c4166.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\n",
      "creating metadata file for C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\1ff5597b098cfe2ee6d3b0c4b3e94e549fcb86f2e033146119d1591f6e4c4166.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\n",
      "loading file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/spiece.model from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\b3949a7257f9b4eaaf4f7785be079d89e4fec7d1c3763a58e6a2743635877d1f.1acf68c74589da6c7fa3548093824dfc450a54637f4356929bbfea7e294a68f8\n",
      "loading file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/special_tokens_map.json from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\1ff5597b098cfe2ee6d3b0c4b3e94e549fcb86f2e033146119d1591f6e4c4166.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\n",
      "loading file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/tokenizer_config.json from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\3f72dece4d3fcab1480f78a2c2a1f6ab591bb9b77ec8b049f8faa418bd50526d.f96255d0af339b1faae306ed2925a98f5266a4ebbf793a8bca5107f6e0f876dd\n",
      "loading configuration file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/config.json from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\d6a784b31cbe212ac3dabbc78bc4e454cf9d8a1b11ff2ef1ba1c7497f2bbfb33.7a3e093739f407bcc025e64dfc5244f91bc573ed4285cba53de90f960cbce58e\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tuner007/pegasus_paraphrase\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/tuner007/pegasus_paraphrase/resolve/main/config.json from cache at C:\\Users\\Keith Martins/.cache\\huggingface\\transformers\\d6a784b31cbe212ac3dabbc78bc4e454cf9d8a1b11ff2ef1ba1c7497f2bbfb33.7a3e093739f407bcc025e64dfc5244f91bc573ed4285cba53de90f960cbce58e\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tuner007/pegasus_paraphrase\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 60,\n",
      "  \"max_position_embeddings\": 60,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "pegasus_ = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c6de4-ef5a-44bb-b309-cddc53e4b2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
