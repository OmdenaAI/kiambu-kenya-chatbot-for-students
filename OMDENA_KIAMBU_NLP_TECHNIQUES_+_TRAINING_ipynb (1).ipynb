{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM_lzb7TqGtw",
    "outputId": "00f5dfeb-28be-4bd0-ae54-ebc2bc3a90bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/loise/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk #Natural languange Processing tool\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "from nltk.stem.lancaster import LancasterStemmer #model for stemming words\n",
    "stemmer = LancasterStemmer()\n",
    "# df = pd.read_excel('/content/Omdena Chatbot Dataset.xlsx')\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQPhCNiDPSxt",
    "outputId": "febfb812-3d00-4b8c-f750-db14df19fdb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hi',\n",
       "    'Hi there',\n",
       "    'Hey',\n",
       "    'hey',\n",
       "    'hi',\n",
       "    'hi there',\n",
       "    'how are you',\n",
       "    'How are you',\n",
       "    'sasa',\n",
       "    'SaSa',\n",
       "    'sema',\n",
       "    'Sema',\n",
       "    'Habari',\n",
       "    'Habari yako',\n",
       "    'uko aje',\n",
       "    'Ukoje',\n",
       "    'hello',\n",
       "    'Hello',\n",
       "    'Good evening',\n",
       "    'Good morning',\n",
       "    'Good afternoon',\n",
       "    'Morning',\n",
       "    'evening',\n",
       "    'afternoon',\n",
       "    'hello there',\n",
       "    'Hello there',\n",
       "    'mambo',\n",
       "    'Mambo'],\n",
       "   'responses': ['Hi stranger',\n",
       "    'poa',\n",
       "    'Hi too',\n",
       "    'hi too',\n",
       "    'hello',\n",
       "    'Hola',\n",
       "    'hello, how can I help you?',\n",
       "    'Poa',\n",
       "    'Mzuri',\n",
       "    'mzuri']},\n",
       "  {'tag': 'goodbye',\n",
       "   'patterns': ['Bye',\n",
       "    'See you later',\n",
       "    'Goodbye',\n",
       "    'Ok bye',\n",
       "    'Bye Bye',\n",
       "    'bye',\n",
       "    'bye bye',\n",
       "    'okay',\n",
       "    'Okay',\n",
       "    'wakati mwema'],\n",
       "   'responses': ['See you!',\n",
       "    'Have a nice day',\n",
       "    'shinda poa',\n",
       "    'sure bye',\n",
       "    'Sure byeeeee',\n",
       "    'Same to you']},\n",
       "  {'tag': 'thanks',\n",
       "   'patterns': ['Thanks',\n",
       "    'Thank you',\n",
       "    'That was helpful',\n",
       "    'Awesome',\n",
       "    'Thanks for your help',\n",
       "    'asanti',\n",
       "    'thank you',\n",
       "    'thank you so much',\n",
       "    'Nashukuru'],\n",
       "   'responses': ['Happy to help',\n",
       "    'Any time!',\n",
       "    'my pleasure',\n",
       "    'You are welcome',\n",
       "    'Sure, see you then',\n",
       "    'My Pleasure',\n",
       "    'Welcome']},\n",
       "  {'tag': 'noanswer',\n",
       "   'patterns': [],\n",
       "   'responses': [\"Sorry, I can't understand you\",\n",
       "    'Please give me mmore information',\n",
       "    'Kindly elaborate on your issue',\n",
       "    'Provide more information please',\n",
       "    'Pardon please',\n",
       "    'Kindly explain more in new terms']},\n",
       "  {'tag': 'name',\n",
       "   'patterns': ['what is your name?',\n",
       "    'your name please',\n",
       "    'name please',\n",
       "    'Your Name please',\n",
       "    'Can you tell me your name'],\n",
       "   'responses': ['am PrinceBot',\n",
       "    'call me PrinceBot',\n",
       "    'Okay, am PrinceBot',\n",
       "    'My name is PrinceBot',\n",
       "    'PrinceBot and what about your name']},\n",
       "  {'tag': 'need',\n",
       "   'patterns': ['I am..., I want your help',\n",
       "    'My name is...,please help',\n",
       "    'Am called...help me here please'],\n",
       "   'responses': ['Good to know your name, how can I help you',\n",
       "    'Great, how can I be of help to you?',\n",
       "    'Okay, state the issue you want me to help you, please']},\n",
       "  {'tag': 'options',\n",
       "   'patterns': ['Can you give me information about DeKUT?',\n",
       "    'Can you tell me about Dedan Kimathi University of Technology?',\n",
       "    'dekut',\n",
       "    'Dekut',\n",
       "    'DEKUT',\n",
       "    'DeKUT',\n",
       "    'Dedan Kimathi University of Technology',\n",
       "    'Tell me about DeKUT',\n",
       "    'What about DeKUT',\n",
       "    'How is DeKUT',\n",
       "    'how is dekut',\n",
       "    'HOW IS DEKUT',\n",
       "    'Explain to me about dekut',\n",
       "    'Explain to me about DeKUT',\n",
       "    'I want information about DeKUT',\n",
       "    'Tell me about Dedan Kimathi University',\n",
       "    'general information about Dedan Kimathi University',\n",
       "    'General information about Dedan Kimathi University',\n",
       "    'General information about Dedan Kimathi University of Technology'],\n",
       "   'responses': ['Dedan Kimathi University of Technology(DeKUT) is a public, coeducational technological University in Africa-Kenya. First university to be established under the new universities act of 2012 and 8th public university in the country. It has two compuses: Main campus located in Nyeri along Nyahururu-Mweiga road and Nairobi CBD campus. DeKUT offers certificate, diploma, undergraduate, masters and PHD programmes in various field.']},\n",
       "  {'tag': 'location',\n",
       "   'patterns': ['Where is DeKUT located',\n",
       "    'location',\n",
       "    'how can I find Dedan Kimathi University of Technology',\n",
       "    'Location of DeKUT.',\n",
       "    'location of Dedan Kimathi Universitiy of Technology',\n",
       "    'Which city is Dedan Kimathi University of Techology found.',\n",
       "    'where is Dedan Kimathi University of Technology located.'],\n",
       "   'responses': [' DeKUT has two campuses:  Main Campus located in Nyeri county,Kenya along Nyahururu-Mweiga highway.  Nairobi CBD Campus located at Pension Towers in Loita Street, nd, th, th floor.']},\n",
       "  {'tag': 'programmes',\n",
       "   'patterns': ['What are the programmes offerred at Dedan Kimathi University of Technology.',\n",
       "    'Courses offered at DeKUT.',\n",
       "    'Courses taught at DeKUT',\n",
       "    'List of courses at Dedan Kimathi University of Technology.',\n",
       "    'Courses offered at Dedan Kimathi University of Technology',\n",
       "    'Courses offered at Dedan Kimathi University.'],\n",
       "   'responses': ['Postgraduate Programmes',\n",
       "    'Undergraduate Programmes',\n",
       "    'Diploma Courses',\n",
       "    'Certificate Courses']},\n",
       "  {'tag': 'postgraduate',\n",
       "   'patterns': ['What are the postgraduate programmes',\n",
       "    'postgraduate programmes',\n",
       "    'Postgraduate Programmes',\n",
       "    'Postgraduate programmes',\n",
       "    'Postgraduate programmes offered in DeKUT',\n",
       "    'PhD courses',\n",
       "    'masters courses',\n",
       "    'Masters programmes',\n",
       "    'Masters courses'],\n",
       "   'responses': ['PhD in Mechanical Engineering',\n",
       "    'PhD in Geomatics & Geospatial Information Science(GeGIS)',\n",
       "    'PhD in Food Science & Technology',\n",
       "    'PhD in Computer Science',\n",
       "    'PhD in Business Administration',\n",
       "    'Master of Science in Geothermal Energy Technology',\n",
       "    'Master of Science in Telecommunication Engineering']},\n",
       "  {'tag': 'undergraduate',\n",
       "   'patterns': ['What are the undergraduate programmes',\n",
       "    'Degree courses',\n",
       "    'degree courses',\n",
       "    'undergraduate programmes',\n",
       "    'Undergradaute programmes',\n",
       "    'Undergraduate Programmes',\n",
       "    'Undergraduate programmes offered in DeKUT',\n",
       "    'List to me Undergraduate Programmes',\n",
       "    'Undergraduate courses'],\n",
       "   'responses': ['BSc in Mechatronic Engineering',\n",
       "    'BSc in Mechanical Engineering',\n",
       "    'BSc in Computer Science',\n",
       "    'BSc in Acturial Science',\n",
       "    'BSc in Civil Engineering',\n",
       "    'Bachelor in Commerce',\n",
       "    'Bachelor of Purchasing and Supplies Management',\n",
       "    'BSc in Geology',\n",
       "    'BSc in Industrial Chemistry',\n",
       "    'BSc in Information Technology',\n",
       "    'BSc in Nursing (Direct Entry)']},\n",
       "  {'tag': 'diploma',\n",
       "   'patterns': ['Give me diploma programmes at Dedan Kimathi University of Technology',\n",
       "    'diploma programmes',\n",
       "    'Diploma programmes',\n",
       "    'What are the diploma programmes offered at DeKUT?',\n",
       "    'Diploma courses'],\n",
       "   'responses': ['Diploma in Leather Technology',\n",
       "    'Diploma in Information Technology',\n",
       "    'Diploma in Security Management',\n",
       "    'Diploma in Coffee Technology & Cupping',\n",
       "    'Diploma in Building Technology',\n",
       "    'Diploma in Fashion Design and Interior Decoration']},\n",
       "  {'tag': 'certificate',\n",
       "   'patterns': ['Give me certificate programmes at Dedan Kimathi University of Technology',\n",
       "    'certificate programmes',\n",
       "    'Certificate programmes',\n",
       "    'What are the certificate programmes offered at DeKUT?,Certificate courses'],\n",
       "   'responses': ['Certificate in Information Technology',\n",
       "    'Certificate in Building Technology',\n",
       "    'Certificate in Electrical & Electronics & Engineering(Power Option)',\n",
       "    'Certificate in Coffee Technology & Quality Management',\n",
       "    'Certificate in Welding,Metal Works & Design']},\n",
       "  {'tag': 'opportunity',\n",
       "   'patterns': ['What are the opportunities at Dedan Kimathi University of Technology',\n",
       "    'Opportunites at Dedan Kimathi University of Technology',\n",
       "    'Any job opportunites at Dedan Kimathi University of Technology',\n",
       "    'Jobs at Dedan Kimathi University of Technology',\n",
       "    'Any Jobs at DeKUT?',\n",
       "    'Can I get a job at DeKUT',\n",
       "    'How can I get a job at DeKUT',\n",
       "    'How can I get employed at Dedan Kimathi University of Technology',\n",
       "    'employment at DeKUT',\n",
       "    'Any Tender at DeKUT',\n",
       "    'How can I get a tender at Dedan Kimathi University of Technology'],\n",
       "   'responses': ['Job opportunities, internships and Tenders are always posted on the University website. Kindly check for any open opportunities and apply for one that interest you.',\n",
       "    'For Jobs, Internships and Tender available in the institution Kindly check the University website. If any is open, follow the given procedures to apply']},\n",
       "  {'tag': 'club',\n",
       "   'patterns': ['Clubs at Dedan Kimathi University of Technology',\n",
       "    'clubs at DeKUT',\n",
       "    'List of clubs at Dedan Kimathi University',\n",
       "    'How many clubs are at Dedan Kimathi University of Technology',\n",
       "    'clubs',\n",
       "    'Clubs',\n",
       "    'Give me clubs at Dedan Kimathi University of Technology',\n",
       "    'Clubs found at DeKUT'],\n",
       "   'responses': ['International Association Exchange of Students for Technical Experience(IAESTE, Contact)',\n",
       "    'DeKUT Innovators Club(Contact: )',\n",
       "    'Google Developers Students Club_DeKUT (Contact: )',\n",
       "    'DeKUT Ajira Digital Club (Contact: )',\n",
       "    'Data Science and Artificial Intelligence Club (Contact: )',\n",
       "    'Microsoft Learn Students Ambassadors DeKUT Club (Contact: )']}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourData = json.loads(open('resources/job_intents.json').read())\n",
    "ourData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y3lF8-0PUzu"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbxH48vZ2wNZ",
    "outputId": "5e13af4e-588f-4c09-ec6b-26f62aa6f1d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 15:49:08.766042: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-06 15:49:08.775005: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-06 15:49:08.775038: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package punkt to /home/loise/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/loise/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as num\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tensorF\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dwphVw8HqOW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9ha40LjJ657",
    "outputId": "c08f39c0-b625-4a39-a24c-8673203b6164"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/loise/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ll8ljOWuN55T",
    "outputId": "d6c8765d-0c66-41d7-9540-167e79262ae4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/loise/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def _lowercase(obj):\n",
    "    \"\"\" Make dictionary lowercase \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k.lower():_lowercase(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, set, tuple)):\n",
    "        t = type(obj)\n",
    "        return t(_lowercase(o) for o in obj)\n",
    "    elif isinstance(obj, str):\n",
    "        return obj.lower()\n",
    "    else:\n",
    "        return obj.translate(str.maketrans('', '', string.punctuation))\n",
    "ourData = _lowercase(ourData)\n",
    "import pandas as pd\n",
    "import nltk #Natural languange Processing tool\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer #model for stemming words\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yn2Jj00eWr-O"
   },
   "source": [
    "###Creating character/word mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KUNnEOjnWoqv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "for intent in ourData[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        characters = sorted(list(set(pattern)))\n",
    "        n_to_char = {n:char for n, char in enumerate(characters)}\n",
    "        char_to_n = {char:n for n, char in enumerate(characters)}\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        length = len(pattern)\n",
    "        seq_length = 100\n",
    "        for i in range(0, length-seq_length, 1):\n",
    "            sequence = ourData[i:i + seq_length]\n",
    "            label =ourData[i + seq_length]\n",
    "            X.append([char_to_n[char] for char in sequence])\n",
    "            Y.append(char_to_n[label])\n",
    "\n",
    "            X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
    "            X_modified = X_modified / float(len(characters))\n",
    "            Y_modified = np_utils.to_categorical(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LCCfmGagJwCD"
   },
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer() \n",
    "ourClasses = []\n",
    "newWords = []\n",
    "documentX = []\n",
    "documentY = []\n",
    "\n",
    "for intent in ourData[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        ournewTkns = nltk.word_tokenize(pattern)\n",
    "        newWords.extend(ournewTkns)\n",
    "        documentX.append(pattern)\n",
    "        documentY.append(intent[\"tag\"])\n",
    "\n",
    "\n",
    "    if intent[\"tag\"] not in ourClasses:\n",
    "        ourClasses.append(intent[\"tag\"])\n",
    "\n",
    "newWords = [lm.lemmatize(word.lower()) for word in newWords if word not in string.punctuation] \n",
    "newWords = sorted(set(newWords))\n",
    "ourClasses = sorted(set(ourClasses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VRUCyy2wKhNA"
   },
   "outputs": [],
   "source": [
    "trainingData = [] # training list array\n",
    "outEmpty = [0] * len(ourClasses)\n",
    "# bow model\n",
    "for idx, doc in enumerate(documentX):\n",
    "    bagOfwords = []\n",
    "    text = lm.lemmatize(doc.lower())\n",
    "    for word in newWords:\n",
    "        bagOfwords.append(1) if word in text else bagOfwords.append(0)\n",
    "\n",
    "    outputRow = list(outEmpty)\n",
    "    outputRow[ourClasses.index(documentY[idx])] = 1\n",
    "    trainingData.append([bagOfwords, outputRow])\n",
    "\n",
    "random.shuffle(trainingData)\n",
    "trainingData = num.array(trainingData, dtype=object)\n",
    "\n",
    "x = num.array(list(trainingData[:, 0]))\n",
    "y = num.array(list(trainingData[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HWGJ3QdK0LH",
    "outputId": "3dbff692-3db7-4572-fcbe-7b34ff7b56ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               13312     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,543\n",
      "Trainable params: 22,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "5/5 [==============================] - 1s 3ms/step - loss: 2.6281 - accuracy: 0.1203\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 2.2037 - accuracy: 0.3233\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.8467 - accuracy: 0.4211\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.4944 - accuracy: 0.5338\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.1535 - accuracy: 0.6692\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.8715 - accuracy: 0.7444\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.7326 - accuracy: 0.7970\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5817 - accuracy: 0.8045\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.8647\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8872\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.9098\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2591 - accuracy: 0.9323\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1800 - accuracy: 0.9624\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.2407 - accuracy: 0.9098\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1674 - accuracy: 0.9398\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.9549\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9699\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.9774\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9624\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1393 - accuracy: 0.9549\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9248\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.9549\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9774\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1143 - accuracy: 0.9774\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.1023 - accuracy: 0.9624\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9925\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9774\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9850\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9925\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9774\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0481 - accuracy: 0.9925\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.9774\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9774\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.9850\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9774\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9774\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9850\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9699\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9850\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9850\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.9850\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9624\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9624\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9850\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9850\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0453 - accuracy: 0.9850\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.9925\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.9850\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9925\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9850\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9850\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9774\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9925\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9774\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9925\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0299 - accuracy: 0.9925\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9925\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0446 - accuracy: 0.9925\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9925\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9850\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9850\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9925\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9774\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9925\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9850\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.9925\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9925\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9925\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0228 - accuracy: 0.9925\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 0.9925\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9925\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9925\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9925\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0384 - accuracy: 0.9925\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9549\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9699\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9699\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9774\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9925\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0563 - accuracy: 0.9774\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9699\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 0.9925\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.9850\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.9850\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9925\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.9925\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9850\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9925\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.1046 - accuracy: 0.9699\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.9925\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9925\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0442 - accuracy: 0.9925\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0642 - accuracy: 0.9774\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9925\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.9850\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0480 - accuracy: 0.9850\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9850\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9925\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0866 - accuracy: 0.9850\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0450 - accuracy: 0.9774\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9549\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9925\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9774\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9850\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9850\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9925\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1040 - accuracy: 0.9774\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9774\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9925\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9850\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9925\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 0.9850\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0396 - accuracy: 0.9925\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9925\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9925\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9699\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0598 - accuracy: 0.9699\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0311 - accuracy: 0.9850\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9850\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0718 - accuracy: 0.9850\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 9.0238e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0473 - accuracy: 0.9774\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0426 - accuracy: 0.9925\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.9774\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 0.9850\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0643 - accuracy: 0.9774\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9925\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0474 - accuracy: 0.9774\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9774\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.9850\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9774\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9925\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0089 - accuracy: 0.9925\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9925\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9850\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9850\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9925\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 0.9850\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0157 - accuracy: 0.9925\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.9850\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9850\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.9850\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9925\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9850\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0671 - accuracy: 0.9850\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9774\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9774\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0373 - accuracy: 0.9925\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.9925\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0233 - accuracy: 0.9925\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.9925\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9850\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9774\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0125 - accuracy: 0.9925\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9850\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9925\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9850\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9624\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9925\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9850\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9850\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0471 - accuracy: 0.9850\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0332 - accuracy: 0.9850\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9925\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.9850\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9774\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.0841 - accuracy: 0.9774\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9624\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.9774\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 9.4200e-04 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9850\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9925\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9850\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7.3865e-04 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.9850\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9624\n"
     ]
    }
   ],
   "source": [
    "# defining some parameters\n",
    "iShape = (len(x[0]),)\n",
    "oShape = len(y[0])\n",
    "\n",
    "# the deep learning model\n",
    "ourNewModel = Sequential()\n",
    "ourNewModel.add(Dense(128, input_shape=iShape, activation=\"relu\"))\n",
    "ourNewModel.add(Dropout(0.5))\n",
    "ourNewModel.add(Dense(64, activation=\"relu\"))\n",
    "ourNewModel.add(Dropout(0.3))\n",
    "ourNewModel.add(Dense(oShape, activation = \"softmax\"))\n",
    "md = tensorF.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "ourNewModel.compile(loss='categorical_crossentropy',\n",
    "              optimizer=md,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(ourNewModel.summary())\n",
    "hist = ourNewModel.fit(x, y, epochs=200, verbose=1)\n",
    "ourNewModel.save('chatbot_model.h5', hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e6AWl1ywLBH6",
    "outputId": "413725af-95ac-42e0-a0df-a9a9bad1b62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autocorrect in /home/loise/.local/lib/python3.10/site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "import string\n",
    "def ourText(text):\n",
    "  spell = Speller()\n",
    "  newtkns = nltk.word_tokenize(spell(text.translate(str.maketrans('', '', string.punctuation))))\n",
    "  newtkns = [lm.lemmatize(word) for word in newtkns]\n",
    "  return newtkns\n",
    "\n",
    "def wordBag(text, vocab):\n",
    "  newtkns = ourText(text)\n",
    "  bagOwords = [0] * len(vocab)\n",
    "  for w in newtkns:\n",
    "    for idx, word in enumerate(vocab):\n",
    "      if word == w:\n",
    "        bagOwords[idx] = 1\n",
    "  return num.array(bagOwords)\n",
    "\n",
    "def Pclass(text, vocab, labels):\n",
    "  bagOwords = wordBag(text, vocab)\n",
    "  ourResult = ourNewModel.predict(num.array([bagOwords]))[0]\n",
    "  newThresh = 0.2\n",
    "  yp = [[idx, res] for idx, res in enumerate(ourResult) if res > newThresh]\n",
    "\n",
    "  yp.sort(key=lambda x: x[1], reverse=True)\n",
    "  newList = []\n",
    "  for r in yp:\n",
    "    newList.append(labels[r[0]])\n",
    "  return newList\n",
    "\n",
    "def getRes(firstlist, fJson):\n",
    "  tag = firstlist[0]\n",
    "  listOfIntents = fJson[\"intents\"]\n",
    "  for i in listOfIntents:\n",
    "    if i[\"tag\"] == tag:\n",
    "      ourResult = random.choice(i[\"responses\"])\n",
    "      break\n",
    "  return ourResult\n",
    "while True:\n",
    "    newMessage = input(\"\")\n",
    "    intents = Pclass(newMessage, newWords, ourClasses)\n",
    "    ourResult = getRes(intents, ourData)\n",
    "    print(ourResult)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdaKoA631HSs"
   },
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition numpy gTTs sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCDYWCXOqtcs"
   },
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "# convert this text to speech\n",
    "text = \"Python is a great programming language\"\n",
    "engine.say(text)\n",
    "# play the speech\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUc3j7z4150g"
   },
   "outputs": [],
   "source": [
    "def AutoCorrect(text):\n",
    "  spell = Speller()\n",
    "  correct = spell(text.translate(str.maketrans('', '', string.punctuation)))\n",
    "  return correct\n",
    "\n",
    "print(AutoCorrect(\"whay is rhhis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BX99gk1qySRU"
   },
   "outputs": [],
   "source": [
    "!sudo apt update && sudo apt install espeak ffmpeg libespeak1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzepC3hd09zC"
   },
   "outputs": [],
   "source": [
    "from gtts import gTTS #Import Google Text to Speech\n",
    "from IPython.display import Audio #Import Audio method from IPython's Display Class\n",
    "tts = gTTS(\"lets dekut meet for the weekly meeting update\") #Provide the string to convert to speech\n",
    "tts.save('1.wav') #save the string converted to speech as a .wav file\n",
    "sound_file = '1.wav'\n",
    "Audio(sound_file, autoplay=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRyR-gwC1mGl"
   },
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIX2cVyO3UDY"
   },
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "# import transformers\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "tokenizer = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
    "\n",
    "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5):\n",
    "  # tokenize the text to be form of a list of token IDs\n",
    "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "  # generate the paraphrased sentences\n",
    "  outputs = model.generate(\n",
    "    **inputs,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "  )\n",
    "  # decode the generated sentences using the tokenizer to get them back to text\n",
    "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "sentence = \"Kindly elaborate your issue.\"\n",
    "get_paraphrased_sentences(model, tokenizer, sentence, num_beams=3, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm_tZtIP39jp"
   },
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smQ71m-mHsJ5"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "res = TextBlob(\"tell me about dekut\")\n",
    "print(res.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YrHuE4naH_Je"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0K7VGfqepcZX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://81eb1000-42b4-4562-894e-03a2a74a2d6e/assets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/loise/personal projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_+_TRAINING_ipynb (1).ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(outfile, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m files:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump(ourNewModel, files)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m outfile\u001b[39m.\u001b[39;49mclose()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "outfile = \"ourModel.sav\"\n",
    "with open(outfile, 'wb') as files:\n",
    "    pickle.dump(ourNewModel, files)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "H-giqTaHpsKS"
   },
   "outputs": [],
   "source": [
    "with open('classes.pkl', 'wb') as files:\n",
    "    pickle.dump(ourClasses, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "for intent in ourData['intents']:\n",
    "   for pattern in intent['patterns']:\n",
    "   # take each word and tokenize it\n",
    "w = nltk.word_tokenize(pattern)\n",
    "words.extend(w)\n",
    "# adding documents\n",
    "documents.append((w, intent['tag']))\n",
    "\n",
    "# adding classes to our class list\n",
    "if intent['tag'] not in classes:\n",
    "   classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 documents\n",
      "1 classes ['club']\n",
      "105 unique lemmatized words [',', '.', '...', 'a', 'about', 'afternoon', 'aje', 'am', 'any', 'are', 'asanti', 'at', 'awesome', 'bye', 'called', 'can', 'certificate', 'city', 'club', 'course', 'dedan', 'degree', 'dekut', 'diploma', 'employed', 'employment', 'evening', 'explain', 'find', 'for', 'found', 'general', 'get', 'give', 'good', 'goodbye', 'habari', 'hello', 'help', 'helpful', 'here', 'hey', 'hi', 'how', 'i', 'in', 'information', 'is', 'job', 'kimathi', 'later', 'list', 'located', 'location', 'mambo', 'many', 'master', 'me', 'morning', 'much', 'mwema', 'my', 'name', 'nashukuru', 'of', 'offered', 'offerred', 'ok', 'okay', 'opportunites', 'opportunity', 'phd', 'please', 'postgraduate', 'programme', 'sasa', 'see', 'sema', 'so', 'taught', 'technology', 'techology', 'tell', 'tender', 'thank', 'thanks', 'that', 'the', 'there', 'to', 'uko', 'ukoje', 'undergradaute', 'undergraduate', 'universitiy', 'university', 'wa', 'wakati', 'want', 'what', 'where', 'which', 'yako', 'you', 'your']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/loise/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/loise/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "intents = json.loads(open('resources/job_intents.json').read())\n",
    "\n",
    "for intent in intents['intents']:\n",
    "   for pattern in intent['patterns']:\n",
    "\n",
    "        # take each word and tokenize it\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        # adding documents\n",
    "        documents.append((w, intent['tag']))\n",
    "        # adding classes to our class list\n",
    "if intent['tag'] not in classes:\n",
    "    classes.append(intent['tag'])\n",
    "\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words\n",
    "    if w not in ignore_words\n",
    "    ]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    classes = sorted(list(set(classes)))\n",
    "\n",
    "    print(len(documents), \"documents\")\n",
    "\n",
    "    print(len(classes), \"classes\", classes)\n",
    "\n",
    "    print(len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "    pickle.dump(words, open('resources/words.pkl', 'wb'))\n",
    "    pickle.dump(classes, open('resources/classes.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'greeting' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/loise/personal projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_+_TRAINING_ipynb (1).ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# output is a '0'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m output_row \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(output_empty)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m output_row[classes\u001b[39m.\u001b[39;49mindex(doc[\u001b[39m1\u001b[39;49m])] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m training\u001b[39m.\u001b[39mappend([bag, output_row])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loise/personal%20projects/DEKUT/OMDENA_KIAMBU_NLP_TECHNIQUES_%2B_TRAINING_ipynb%20%281%29.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# shuffle our features and turn into np.array\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: 'greeting' is not in list"
     ]
    }
   ],
   "source": [
    "# initializing training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "for doc in documents:\n",
    "      # initializing bag of words\n",
    "   bag = []\n",
    "\n",
    "   pattern_words = doc[0]\n",
    "   # lemmatize each word - create base word, in attempt to represent related words\n",
    "   pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "   # create our bag of words array with 1,\n",
    "   for w in words:\n",
    "      if w in pattern_words:\n",
    "         bag.append(1) \n",
    "\n",
    "      else:\n",
    "        bag.append(0)\n",
    "\n",
    "   # output is a '0'\n",
    "\n",
    "   output_row = list(output_empty)\n",
    "   output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "   training.append([bag, output_row])\n",
    "   # shuffle our features and turn into np.array\n",
    "   random.shuffle(training)\n",
    "   training = np.array(training)\n",
    "   # create train and test lists.X - patterns, Y - intents\n",
    "   train_x = list(training[: , 0])\n",
    "   train_y = list(training[: , 1])\n",
    "   print(\"Training data created\")\n",
    "\n",
    "\n",
    "   # Create model - 3 layers.First layer 128 neurons, second layer 64 neurons and 3 rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape = (len(train_x[0]), ), activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation = 'softmax'))\n",
    "\n",
    "\n",
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "\n",
    "#fitting and saving the model\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs = 200, batch_size = 5, verbose = 1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OMDENA KIAMBU NLP TECHNIQUES + TRAINING.  ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
